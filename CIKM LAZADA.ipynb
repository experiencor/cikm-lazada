{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:58:24.735314Z",
     "start_time": "2017-05-11T22:58:24.714711+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn import svm\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "word_parser = RegexpTokenizer('[A-Za-z]+', flags=re.UNICODE)\n",
    "digit_checker = re.compile(\"\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:44:52.646022Z",
     "start_time": "2017-05-11T22:44:52.487739+08:00"
    },
    "code_folding": [
     0,
     6,
     19,
     44,
     51,
     59,
     65,
     73,
     80,
     91
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_submission(filename, predicted_results):\n",
    "    if not os.path.exists('submission'):\n",
    "        os.makedirs('submission')\n",
    "    np.savetxt('submission/' + filename, predicted_results, fmt='%.5f')\n",
    "    print(filename + ' updated!')\n",
    "\n",
    "def tokenize_description(description):\n",
    "    description = BeautifulSoup(description)\n",
    "    description = description.getText(' ')\n",
    "    \n",
    "    tokens = word_parser.tokenize(description)\n",
    "    \n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = list(filter(lambda t: t not in stop, tokens))\n",
    "    tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "    tokens = list(filter(lambda t: t not in [u'x'], tokens))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize_title(title):\n",
    "    try:\n",
    "        title = ''.join(i for i in title if ord(i)<128)\n",
    "        tokens_ = [word_tokenize(sent) for sent in sent_tokenize(title)]\n",
    "        \n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "        tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``', u'\\u2014', u'\\u2026', u'\\u2013'], tokens))\n",
    "        \n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "\n",
    "        filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    "\n",
    "        return filtered_tokens\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def has_non_ascii(title):        \n",
    "    try:\n",
    "        title.decode('ascii')\n",
    "        return 0\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "def has_number(title): \n",
    "    title = title.decode('utf-8').encode('ascii', errors='ignore')\n",
    "    \n",
    "    if digit_checker.search(title):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0      \n",
    "    \n",
    "def has_slash(title): \n",
    "    if '\\\\' in title:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "        \n",
    "def has_duplicates(values):\n",
    "    # For each element, check all following elements for a duplicate.\n",
    "    for i in range(0, len(values)):\n",
    "        for x in range(i + 1, len(values)):\n",
    "            if values[i] == values[x]:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def calculate_tfidf_sum(tokens):\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if tfidf.tfidf.get(token) is not None:\n",
    "            score = score + tfidf.tfidf.get(token)\n",
    "    return score\n",
    "\n",
    "def calculate_tfidf_avg(tokens):\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if tfidf.tfidf.get(token) is not None:\n",
    "            score = score + tfidf.tfidf.get(token)\n",
    "    if len(tokens)>1:\n",
    "        mean = score/len(tokens)\n",
    "    else:\n",
    "        mean = 0\n",
    "    return mean\n",
    "\n",
    "def keyword_density(title):\n",
    "    count = 0\n",
    "    length = len(title)\n",
    "    for word in title:\n",
    "        if word in tfidf.index:\n",
    "            count += 1\n",
    "    density = count/length if count>1 else 0\n",
    "    return density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:44:55.369948Z",
     "start_time": "2017-05-11T22:44:55.075508+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(filepath_or_buffer='data/training/data_train.csv', \n",
    "                 names=['country','sku_id','title','category_lvl_1','category_lvl_2','category_lvl_3','short_description','price','product_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:44:56.211392Z",
     "start_time": "2017-05-11T22:44:56.173535+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>sku_id</th>\n",
       "      <th>title</th>\n",
       "      <th>category_lvl_1</th>\n",
       "      <th>category_lvl_2</th>\n",
       "      <th>category_lvl_3</th>\n",
       "      <th>short_description</th>\n",
       "      <th>price</th>\n",
       "      <th>product_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my</td>\n",
       "      <td>AD674FAASTLXANMY</td>\n",
       "      <td>Adana Gallery Suri Square Hijab – Light Pink</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>Women</td>\n",
       "      <td>Muslim Wear</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Material : Non sheer shimmer chiffon&lt;/...</td>\n",
       "      <td>49.00</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my</td>\n",
       "      <td>AE068HBAA3RPRDANMY</td>\n",
       "      <td>Cuba Heartbreaker Eau De Parfum Spray 100ml/3.3oz</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "      <td>Bath &amp; Body</td>\n",
       "      <td>Hand &amp; Foot Care</td>\n",
       "      <td>Formulated with oil-free hydrating botanicals/...</td>\n",
       "      <td>128.00</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my</td>\n",
       "      <td>AN680ELAA9VN57ANMY</td>\n",
       "      <td>Andoer 150cm Cellphone Smartphone Mini Dual-He...</td>\n",
       "      <td>TV, Audio / Video, Gaming &amp; Wearables</td>\n",
       "      <td>Audio</td>\n",
       "      <td>Live Sound &amp; Stage</td>\n",
       "      <td>&lt;ul&gt; &lt;li&gt;150cm mini microphone compatible for ...</td>\n",
       "      <td>25.07</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my</td>\n",
       "      <td>AN957HBAAAHDF4ANMY</td>\n",
       "      <td>ANMYNA Complaint Silky Set 柔顺洗发配套 (Shampoo 520...</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "      <td>Hair Care</td>\n",
       "      <td>Shampoos &amp; Conditioners</td>\n",
       "      <td>&lt;ul&gt; &lt;li&gt;ANMYNA Complaint Silky Set (Shampoo 5...</td>\n",
       "      <td>118.00</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my</td>\n",
       "      <td>AR511HBAXNWAANMY</td>\n",
       "      <td>Argital Argiltubo Green Clay For Face and Body...</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "      <td>Men's Care</td>\n",
       "      <td>Body and Skin Care</td>\n",
       "      <td>&lt;ul&gt; &lt;li&gt;100% Authentic&lt;/li&gt; &lt;li&gt;Rrefresh and ...</td>\n",
       "      <td>114.80</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country              sku_id  \\\n",
       "0      my    AD674FAASTLXANMY   \n",
       "1      my  AE068HBAA3RPRDANMY   \n",
       "2      my  AN680ELAA9VN57ANMY   \n",
       "3      my  AN957HBAAAHDF4ANMY   \n",
       "4      my    AR511HBAXNWAANMY   \n",
       "\n",
       "                                               title  \\\n",
       "0       Adana Gallery Suri Square Hijab – Light Pink   \n",
       "1  Cuba Heartbreaker Eau De Parfum Spray 100ml/3.3oz   \n",
       "2  Andoer 150cm Cellphone Smartphone Mini Dual-He...   \n",
       "3  ANMYNA Complaint Silky Set 柔顺洗发配套 (Shampoo 520...   \n",
       "4  Argital Argiltubo Green Clay For Face and Body...   \n",
       "\n",
       "                          category_lvl_1 category_lvl_2  \\\n",
       "0                                Fashion          Women   \n",
       "1                        Health & Beauty    Bath & Body   \n",
       "2  TV, Audio / Video, Gaming & Wearables          Audio   \n",
       "3                        Health & Beauty      Hair Care   \n",
       "4                        Health & Beauty     Men's Care   \n",
       "\n",
       "            category_lvl_3                                  short_description  \\\n",
       "0              Muslim Wear  <ul><li>Material : Non sheer shimmer chiffon</...   \n",
       "1         Hand & Foot Care  Formulated with oil-free hydrating botanicals/...   \n",
       "2       Live Sound & Stage  <ul> <li>150cm mini microphone compatible for ...   \n",
       "3  Shampoos & Conditioners  <ul> <li>ANMYNA Complaint Silky Set (Shampoo 5...   \n",
       "4       Body and Skin Care  <ul> <li>100% Authentic</li> <li>Rrefresh and ...   \n",
       "\n",
       "    price   product_type  \n",
       "0   49.00          local  \n",
       "1  128.00  international  \n",
       "2   25.07  international  \n",
       "3  118.00          local  \n",
       "4  114.80  international  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:45:34.858675Z",
     "start_time": "2017-05-11T22:45:24.893114+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute tf-idf on descriptions\n",
    "tf_idf_desription = TfidfVectorizer(min_df=10, max_features=10000, tokenizer=tokenize_description, ngram_range=(1, 2))\n",
    "tf_idf_desription.fit(list(df_train['short_description'].replace(np.nan, '')))\n",
    "\n",
    "# Compute tf-idf on titles\n",
    "tf_idf_title = TfidfVectorizer(min_df=10, max_features=10000, tokenizer=tokenize_title, ngram_range=(1, 2))\n",
    "tf_idf_title.fit(list(df_train['title']))\n",
    "\n",
    "tfidf = dict(zip(tf_idf_title.get_feature_names(), tf_idf_title.idf_))\n",
    "tfidf = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidf), orient='index')\n",
    "tfidf.columns = ['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:54:00.173688Z",
     "start_time": "2017-05-11T22:54:00.116583+08:00"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_features(data_frame):\n",
    "    descriptions = tf_idf_desription.transform(list(data_frame['short_description'].replace(np.nan, '')))\n",
    "    titles = tf_idf_title.transform(list(data_frame['title']))\n",
    "    data_frame['tokenized_title'] = df_train['title'].map(tokenize_title)\n",
    "    \n",
    "    # Compute title length\n",
    "    if 'token_num' not in data_frame:\n",
    "        data_frame['token_num'] = data_frame['tokenized_title'].map(len)\n",
    "\n",
    "    # Compute new feature - 1 if title has duplicates, 0 if not.\n",
    "    if 'has_duplicate' not in data_frame:\n",
    "        data_frame['has_duplicate'] = data_frame['tokenized_title'].map(has_duplicates)\n",
    "\n",
    "    # Compute tf-idf sum\n",
    "    if 'tfidf_sum' not in data_frame:\n",
    "        data_frame['tfidf_sum'] = data_frame['tokenized_title'].map(calculate_tfidf_sum)\n",
    "\n",
    "    # Compute tf-idf average\n",
    "    if 'tfidf_avg' not in data_frame:\n",
    "        data_frame['tfidf_avg'] = data_frame['tokenized_title'].map(calculate_tfidf_avg)\n",
    "\n",
    "    # Compute title length\n",
    "    if 'title_length' not in data_frame:\n",
    "        data_frame['title_length'] = data_frame['title'].map(len)\n",
    "\n",
    "    # Compute new feature - 1 if title has non-ascii character, 0 otherwise\n",
    "    if 'has_non_ascii' not in data_frame:\n",
    "        data_frame['has_non_ascii'] = data_frame['title'].map(has_non_ascii)\n",
    "\n",
    "    # Compute new feature - 1 if title has a number, 0 otherwise\n",
    "    if 'has_number' not in data_frame:\n",
    "        data_frame['has_number'] = data_frame['title'].map(has_number)\n",
    "\n",
    "    # Compute new feature - 1 if title has a backslash, 0 otherwise\n",
    "    if 'has_backslash' not in data_frame:\n",
    "        data_frame['has_backslash'] = data_frame['title'].map(has_number)\n",
    "    \n",
    "    X = data_frame[['token_num', \n",
    "                  'has_duplicate', \n",
    "                  'tfidf_sum', \n",
    "                  'tfidf_avg', \n",
    "                  'title_length', \n",
    "                  'has_non_ascii', \n",
    "                  'has_number',\n",
    "                  'has_backslash']]\n",
    "\n",
    "    X = np.concatenate([X.as_matrix(),\n",
    "                        titles.toarray(),\n",
    "                        descriptions.toarray(),\n",
    "                        pd.get_dummies(data_frame['category_lvl_1']).as_matrix(), \n",
    "                        pd.get_dummies(data_frame['category_lvl_2']).as_matrix(),\n",
    "                        pd.get_dummies(data_frame['category_lvl_3']).as_matrix(),\n",
    "                        pd.get_dummies(data_frame['product_type']).as_matrix(),\n",
    "                        data_frame['price'].as_matrix().reshape(-1,1),\n",
    "                        (data_frame.product_type == 'local').as_matrix().astype(float).reshape(-1,1)\n",
    "                       ], \n",
    "                       axis=1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Conciseness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:52:05.349353Z",
     "start_time": "2017-05-11T22:51:26.321780+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = construct_features(df_train)\n",
    "y = pd.read_csv(\"data/training/conciseness_train.labels\", header=None).as_matrix().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:52:15.181617Z",
     "start_time": "2017-05-11T22:52:10.486580+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RMSE: 0.349795\n"
     ]
    }
   ],
   "source": [
    "# SPLIT INTO TRAINING SET AND VALIDATION SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# TRAIN AND EVALUATE THE MODEL\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model RMSE: %f\" % mean_squared_error(model.predict_proba(X_test)[:,1], y_test)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-11T22:58:34.608Z"
    }
   },
   "outputs": [],
   "source": [
    "# SPLIT INTO TRAINING SET AND VALIDATION SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# TRAIN AND EVALUATE THE MODEL\n",
    "model = svm.SVR()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model RMSE: %f\" % mean_squared_error(model.predict_proba(X_test)[:,1], y_test)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:54:28.768968Z",
     "start_time": "2017-05-11T22:54:05.430261+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv(filepath_or_buffer='data/validation/data_valid.csv', \n",
    "                       names=['country','sku_id','title','category_lvl_1','category_lvl_2','category_lvl_3','short_description','price','product_type'])\n",
    "\n",
    "X_valid = construct_features(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:54:34.514047Z",
     "start_time": "2017-05-11T22:54:30.879853+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conciseness_valid.predict updated!\n"
     ]
    }
   ],
   "source": [
    "# RETRAIN THE MODEL ON THE WHOLE DATASET\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "predicted_results = model.predict_proba(X_valid)[:, 1]\n",
    "write_submission('conciseness_valid.predict', predicted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:54:34.520808Z",
     "start_time": "2017-05-11T22:54:34.515576+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONSTRUCT INPUTS AND OUTPUTS\n",
    "y = pd.read_csv(\"data/training/clarity_train.labels\", header=None).as_matrix().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:54:37.726865Z",
     "start_time": "2017-05-11T22:54:34.522186+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SPLIT INTO TRAINING SET AND VALIDATION SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:54:41.181060Z",
     "start_time": "2017-05-11T22:54:37.728325+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RMSE: 0.218032\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND EVALUATE THE MODEL\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model RMSE: %f\" % mean_squared_error(model.predict_proba(X_test)[:,1], y_test)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:54:45.845486Z",
     "start_time": "2017-05-11T22:54:41.182399+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clarity_valid.predict updated!\n"
     ]
    }
   ],
   "source": [
    "# RETRAIN THE MODEL ON THE WHOLE DATASET\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "predicted_results = model.predict_proba(X_valid)[:, 1]\n",
    "write_submission('clarity_valid.predict', predicted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T14:54:45.931274Z",
     "start_time": "2017-05-11T22:54:45.846800+08:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('zip -j submission submission/*')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "67px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
